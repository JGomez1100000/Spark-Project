{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wget pysparkâ€¯ findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/25 13:14:27 WARN Utils: Your hostname, javier-virtual-machine resolves to a loopback address: 127.0.1.1; using 192.168.81.128 instead (on interface ens33)\n",
      "24/04/25 13:14:27 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/04/25 13:14:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Creating a SparkContext object\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "# Creating a Spark Session\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark DataFrames basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dataset2.csv'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wget\n",
    "\n",
    "link_to_data1 = 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/labs/data/dataset1.csv'\n",
    "wget.download(link_to_data1)\n",
    "\n",
    "link_to_data2 = 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/labs/data/dataset2.csv'\n",
    "wget.download(link_to_data2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data into a pyspark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df1 = spark.read.csv(\"dataset1.csv\", header=True, inferSchema=True)\n",
    "df2 = spark.read.csv(\"dataset2.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print the schema of df1 and df2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- date_column: string (nullable = true)\n",
      " |-- amount: integer (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- transaction_date: string (nullable = true)\n",
      " |-- value: integer (nullable = true)\n",
      " |-- notes: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.printSchema()\n",
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year, quarter, to_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add new column year to df1\n",
    "df1 = df1.withColumn('year', year(to_date('date_column','dd/MM/yyyy')))\n",
    "\n",
    "#Add new column quarter to df2    \n",
    "df2 = df2.withColumn('quarter', quarter(to_date('transaction_date','dd/MM/yyyy')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rename df1 column amount to transaction_amount\n",
    "df1 = df1.withColumnRenamed('amount', 'transaction_amount')\n",
    "\n",
    "#Rename df2 column value to transaction_value\n",
    "df2 = df2.withColumnRenamed('value', 'transaction_value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop columns description and location from df1\n",
    "df1 = df1.drop('description', 'location')\n",
    "\n",
    "#Drop column notes from df2\n",
    "df2 = df2.drop('notes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#join df1 and df2 based on common column customer_id\n",
    "joined_df = df1.join(df2, 'customer_id', 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter the dataframe for transaction amount > 1000\n",
    "filtered_df = joined_df.filter(\"transaction_amount > 1000\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+\n",
      "|customer_id|total_amount|\n",
      "+-----------+------------+\n",
      "|         31|        3200|\n",
      "|         85|        1800|\n",
      "|         78|        1500|\n",
      "|         34|        1200|\n",
      "|         81|        5500|\n",
      "|         28|        2600|\n",
      "|         76|        2600|\n",
      "|         27|        4200|\n",
      "|         91|        3200|\n",
      "|         22|        1200|\n",
      "|         93|        5500|\n",
      "|          1|        5000|\n",
      "|         52|        2600|\n",
      "|         13|        4800|\n",
      "|          6|        4500|\n",
      "|         16|        2600|\n",
      "|         40|        2600|\n",
      "|         94|        1200|\n",
      "|         57|        5500|\n",
      "|         54|        1500|\n",
      "+-----------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "\n",
    "# group by customer_id and aggregate the sum of transaction amount\n",
    "\n",
    "total_amount_per_customer = filtered_df.groupBy('customer_id').agg(sum('transaction_amount').alias('total_amount'))\n",
    "\n",
    "#display the result\n",
    "total_amount_per_customer.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Write total_amount_per_customer to a Hive table named customer_totals\n",
    "total_amount_per_customer.write.mode(\"overwrite\").saveAsTable(\"customer_totals\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Write filtered_df to HDFS in parquet format file filtered_data\n",
    "#Change spark date format  configuration\n",
    "\n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "\n",
    "filtered_df.write.mode(\"overwrite\").parquet(\"filtered_data.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, lit\n",
    "\n",
    "# Add new column with value indicating whether transaction amount is > 5000 or not\n",
    "df1 = df1.withColumn(\"high_value\", when(df1.transaction_amount > 5000, lit(\"Yes\")).otherwise(lit(\"No\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 27:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+\n",
      "|quarter|avg_trans_val|\n",
      "+-------+-------------+\n",
      "|      1|       1111.0|\n",
      "|      2|       1072.0|\n",
      "|      3|       1958.0|\n",
      "|      4|        817.0|\n",
      "+-------+-------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg, round\n",
    "\n",
    "#calculate the average transaction value for each quarter in df2\n",
    "average_value_per_quarter = (df2.groupBy('quarter').agg(round(avg(\"transaction_value\")).alias(\"avg_trans_val\"))).orderBy(\"quarter\")\n",
    "#show the average transaction value for each quarter in df2    \n",
    "average_value_per_quarter.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Write average_value_per_quarter to a Hive table named quarterly_averages\n",
    "\n",
    "average_value_per_quarter.write.mode(\"overwrite\").saveAsTable(\"quarterly_averages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------------------+\n",
      "|year|total_transaction_val|\n",
      "+----+---------------------+\n",
      "|2022|                29800|\n",
      "|2023|                28100|\n",
      "|2024|                25700|\n",
      "|2025|                25700|\n",
      "|2026|                25700|\n",
      "|2027|                25700|\n",
      "|2028|                25700|\n",
      "|2029|                25700|\n",
      "|2030|                 9500|\n",
      "+----+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# calculate the total transaction value for each year in df1.\n",
    "total_value_per_year = (df1.groupBy('year').agg(sum(\"transaction_amount\").alias(\"total_transaction_val\"))).orderBy(\"year\")\n",
    "\n",
    "# show the total transaction value for each year in df1.\n",
    "total_value_per_year.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Write total_value_per_year to HDFS in the CSV format\n",
    "\n",
    "total_value_per_year.write.mode(\"overwrite\").csv(\"total_value_per_year.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
